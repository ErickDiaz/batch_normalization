# batch_normalization
Batch Normalization Batch normalization is a technique for improving the performance and stability of neural networks. The idea is to normalize the layer inputs such that they have a mean of zero and variance of one, much like how we standardize the inputs to networks. Batch normalization is necessary to make DCGANs work.
